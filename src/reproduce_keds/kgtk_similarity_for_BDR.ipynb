{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from database import db, cursor\n",
    "from util import json_load, json_dump\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import csv\n",
    "from multiprocessing import Pool\n",
    "from concurrent import futures\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://kgtk.isi.edu/similarity_api'\n",
    "headers = {'User-Agent': 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'} \n",
    "proxies = {'http': 'http://114.212.82.174:10809', 'https': 'http://114.212.82.174:10809'}\n",
    "logging.basicConfig(filename='kgtk_similarity.log', level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_semantic_similarity(input_file):\n",
    "    file_name = os.path.basename(input_file)\n",
    "    files = {\n",
    "        'file': (file_name, open(input_file, mode='rb'), 'application/octet-stream')\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.post(url, files=files, params={'similarity_types': 'text'}, proxies=proxies, headers=headers)\n",
    "        s = json.loads(resp.json())\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error in calling {url} with {input_file}: {e}')\n",
    "        s = []\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dataset_entity = defaultdict(list)\n",
    "cursor.execute('SELECT dataset_id, URI FROM `BDR_metadata_link_result_FALCON2` WHERE URI is not NULL;')\n",
    "for dataset_id, wikidata_uri in cursor.fetchall():\n",
    "    dataset_id = int(dataset_id)\n",
    "    wikidata_uri = wikidata_uri.split('/')[-1]\n",
    "    dataset_entity[dataset_id].append(wikidata_uri)\n",
    "\n",
    "query_entity = defaultdict(list)\n",
    "cursor.execute('SELECT query_id, URI FROM `BDR_query_link_result_FALCON2` WHERE URI is not NULL;')\n",
    "for query_id, wikidata_uri in cursor.fetchall():\n",
    "    query_id = str(query_id)\n",
    "    wikidata_uri = wikidata_uri.split('/')[-1]\n",
    "    query_entity[query_id].append(wikidata_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算所有可能的pair\n",
    "pair_set = set()\n",
    "sparse_res_path = f''\n",
    "sparse_res = json_load(sparse_res_path)\n",
    "with tqdm(total=len(sparse_res), ncols=100, leave=True) as pbar:\n",
    "    for query_id, dataset_id in sparse_res:\n",
    "        query_entity_list = query_entity[query_id]\n",
    "        dataset_entity_list = dataset_entity[dataset_id]\n",
    "        for q, d in itertools.product(query_entity_list, dataset_entity_list):\n",
    "            pair_set.add((q, d))\n",
    "    pbar.update(1)\n",
    "    \n",
    "with open('kgtk_BDR_pairs.tsv', 'w') as f:\n",
    "    for pair in pair_set:\n",
    "        f.write(f'{pair[0]}\\t{pair[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "error_list = []\n",
    "def single_run(unhandled_list=None):\n",
    "    global target_list\n",
    "    pair_list = []\n",
    "    similarity_path = f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/similarity/slices'\n",
    "    pair_path = f'kgtk_BDR_pairs.tsv'\n",
    "    for row in csv.reader(open(pair_path, 'r'), delimiter='\\t'):\n",
    "        pair_list.append((row[0], row[1]))\n",
    "    \n",
    "    print('pair len:', len(pair_list))\n",
    "    l = 25\n",
    "\n",
    "    if unhandled_list:\n",
    "        for index in unhandled_list:\n",
    "            index = int(index)\n",
    "            with open('tmp.tsv', 'w+') as fp:\n",
    "                fp.write('q1\\tq2\\n')\n",
    "                for pair in pair_list[index:index+l]:\n",
    "                    fp.write(f'{pair[0]}\\t{pair[1]}\\n')\n",
    "            try:\n",
    "                s = call_semantic_similarity('tmp.tsv')\n",
    "                s = [dict(t) for t in set([tuple(d.items()) for d in s])]\n",
    "                assert len(s) == l\n",
    "            except Exception as e:\n",
    "                logging.error(f'Error in calling {url} with [{index}:{index+l}]: {e} len s: {len(s)}')\n",
    "                error_list.append(index)\n",
    "                s = []\n",
    "            target_list.extend(s)\n",
    "            logging.info(f'finish {index + l}')\n",
    "        json_dump(target_list, f'/home/xxx/code/keds/kgtk_similarity/similarity/all_unhandled_similarity.json')\n",
    "        return\n",
    "    \n",
    "    with tqdm(total=len(pair_list), ncols=100, leave=True) as pbar:\n",
    "        for i in range(110000, len(pair_list), l):\n",
    "            with open('tmp.tsv', 'w+') as fp:\n",
    "                fp.write('q1\\tq2\\n')\n",
    "                for pair in pair_list[i:i+l]:\n",
    "                    fp.write(f'{pair[0]}\\t{pair[1]}\\n')\n",
    "            try:\n",
    "                s = call_semantic_similarity('tmp.tsv')\n",
    "                s = [dict(t) for t in set([tuple(d.items()) for d in s])]\n",
    "                assert len(s) == len(pair_list[i:i+l])\n",
    "            except Exception as e:\n",
    "                logging.error(f'Error in calling {url} with [{i}:{i+l}]: {e} len s: {len(s)}')\n",
    "                error_list.append(i)\n",
    "                s = []\n",
    "            target_list.extend(s)\n",
    "            logging.info(f'finish {i + l}')\n",
    "            if int(i + l) % 10000 == 0:\n",
    "                target_list = [dict(t) for t in set([tuple(d.items()) for d in target_list])]\n",
    "                json_dump(target_list[i + l - 10000: i + l], f'{similarity_path}/{i + l}.json')\n",
    "            pbar.update(l)\n",
    "    target_list = [dict(t) for t in set([tuple(d.items()) for d in target_list])]\n",
    "    json_dump(target_list, f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/similarity/all_similarity.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sim = []\n",
    "similarity_path = f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/similarity/slices'\n",
    "for file in os.listdir(similarity_path):\n",
    "    all_sim.extend(json_load(f'{similarity_path}/{file}'))\n",
    "\n",
    "json_dump(all_sim, f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/similarity/BDR_all_similarity.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "unhandled_list = []\n",
    "with open('/home/xxx/code/reproduce_keds/kgtk_similarity.log', 'r+') as fp:\n",
    "    for line in fp.readlines():\n",
    "        if 'len s:' in line:\n",
    "            error = re.search(r'\\[\\d+:\\d+\\]', line).group(0)\n",
    "            unhandled_list.append(error[1:-1].split(':')[0])\n",
    "print(unhandled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_run(unhandled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_path = f'/home/xxx/code/keds/kgtk_similarity/similarity/{test_collection}_rank{rank}_similarity.json'\n",
    "src_path = '/home/xxx/code/keds/kgtk_similarity/similarity/all_similarity.json'\n",
    "\n",
    "data = json_load(src_path)\n",
    "row_list = []\n",
    "for item in data:\n",
    "    query_entity_uri = 'http://www.wikidata.org/entity/' + item['q1']\n",
    "    dataset_entity_uri = 'http://www.wikidata.org/entity/' + item['q2']\n",
    "    query_entity_label = item['q1_label'] if item['q1_label'] else ''\n",
    "    dataset_entity_label = item['q2_label'] if item['q2_label'] else ''\n",
    "    text_similarity = item['text'] if item['text'] else -1\n",
    "    row_list.append((query_entity_uri, dataset_entity_uri, query_entity_label, dataset_entity_label, float(text_similarity)))\n",
    "\n",
    "# ntcir_cursor_org.executemany(f'INSERT  INTO {test_collection}_query_dataset_entity_similartity_kgtk \\\n",
    "                        #  (query_entity_uri, dataset_entity_uri, query_entity_label, dataset_entity_label, text_similarity ) \\\n",
    "                        #  VALUES (%s,%s,%s,%s,%s);', row_list)\n",
    "cursor.executemany(f'INSERT  INTO query_dataset_entity_similartity_kgtk \\\n",
    "                         (query_entity_uri, dataset_entity_uri, query_entity_label, dataset_entity_label, text_similarity ) \\\n",
    "                         VALUES (%s,%s,%s,%s,%s);', row_list)\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
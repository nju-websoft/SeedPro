{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from database import db, cursor\n",
    "from util import json_load, json_dump\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import csv\n",
    "from multiprocessing import Pool\n",
    "from concurrent import futures\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://kgtk.isi.edu/similarity_api'\n",
    "headers = {'User-Agent': 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'} \n",
    "proxies = {'http': 'http://114.212.82.174:10809', 'https': 'http://114.212.82.174:10809'}\n",
    "logging.basicConfig(filename='kgtk_similarity.log', level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_semantic_similarity(input_file):\n",
    "    file_name = os.path.basename(input_file)\n",
    "    files = {\n",
    "        'file': (file_name, open(input_file, mode='rb'), 'application/octet-stream')\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.post(url, files=files, params={'similarity_types': 'text'}, proxies=proxies, headers=headers)\n",
    "        s = json.loads(resp.json())\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error in calling {url} with {input_file}: {e}')\n",
    "        s = []\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "cursor.execute('SELECT dataset_id, entity_id FROM `acordar1_metadata_NPR` WHERE entity_id is not NULL;')\n",
    "acordar1_dataset_entity = defaultdict(list)\n",
    "for (dataset_id, entity_id) in cursor.fetchall():\n",
    "    acordar1_dataset_entity[int(dataset_id)].append(entity_id)\n",
    "\n",
    "cursor.execute('SELECT dataset_id, entity_id FROM `ntcir_metadata_NPR` WHERE entity_id is not NULL;')\n",
    "ntcir_dataset_entity = defaultdict(list)\n",
    "for (dataset_id, entity_id) in cursor.fetchall():\n",
    "    ntcir_dataset_entity[int(dataset_id)].append(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acordar1_query_entity = defaultdict(list)\n",
    "cursor.execute('SELECT query_id, entity_id FROM `acordar1_query_NPR` WHERE entity_id is not NULL;')\n",
    "for (query_id, entity_id) in cursor.fetchall():\n",
    "    acordar1_query_entity[str(query_id)].append(entity_id)\n",
    "\n",
    "ntcir15_query_entity = defaultdict(list)\n",
    "cursor.execute('SELECT query_id, entity_id FROM `ntcir15_query_NPR` WHERE entity_id is not NULL;')\n",
    "for (query_id, entity_id) in cursor.fetchall():\n",
    "    ntcir15_query_entity[str(query_id)].append(entity_id)\n",
    "\n",
    "ntcir16_query_entity = defaultdict(list)\n",
    "cursor.execute('SELECT query_id, entity_id FROM `ntcir16_query_NPR` WHERE entity_id is not NULL;')\n",
    "for (query_id, entity_id) in cursor.fetchall():\n",
    "    ntcir16_query_entity[str(query_id)].append(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(acordar1_dataset_entity), len(ntcir_dataset_entity), len(acordar1_query_entity), len(ntcir15_query_entity), len(ntcir16_query_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acordar1_dataset_entity.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_list = []\n",
    "for test_collection in ['acordar1', 'ntcir15', 'ntcir16']:\n",
    "    print('start', test_collection)\n",
    "    sparse_res_path = f'/home/xxx/code/erm/data/retrieve_results/{test_collection}/candidates/BM25 [m] test_top100_sorted.json'\n",
    "    sparse_res = json_load(sparse_res_path)\n",
    "    for key, value in sparse_res.items():\n",
    "        tmp_list = []\n",
    "        for i in value[:10]:\n",
    "            tmp_list.append(str(i[0]))\n",
    "        sparse_res[key] = tmp_list\n",
    "\n",
    "    with tqdm(total=len(sparse_res), ncols=100, leave=True) as pbar:\n",
    "        for query_id, dataset_id_list in sparse_res.items():\n",
    "            # cursor.execute(f'SELECT entity_id FROM {test_collection}_query_NPR WHERE query_id = \"{query_id}\";')\n",
    "            query_uri_list = []\n",
    "            if test_collection.startswith('ntcir15'):\n",
    "                query_uri_list.extend(ntcir15_query_entity[query_id])\n",
    "            elif test_collection.startswith('ntcir16'):\n",
    "                query_uri_list.extend(ntcir16_query_entity[query_id])\n",
    "            else:\n",
    "                query_uri_list.extend(acordar1_query_entity[query_id])\n",
    "            query_uri_list = list(set(query_uri_list))\n",
    "\n",
    "            # print('query uri len:', len(query_uri_list))\n",
    "            dataset_uri_list = []\n",
    "            for dataset_id in dataset_id_list:\n",
    "                if test_collection.startswith('ntcir'):\n",
    "                    dataset_uri_list.extend(ntcir_dataset_entity[int(dataset_id)])\n",
    "                else:\n",
    "                    dataset_uri_list.extend(acordar1_dataset_entity[int(dataset_id)])\n",
    "            dataset_uri_list = list(set(dataset_uri_list))\n",
    "            # dataset_uri_list = list([i[0] for i in cursor.fetchall()])\n",
    "            # print('dataset uri len:', len(dataset_uri_list))\n",
    "            for query_uri, dataset_uri in itertools.product(query_uri_list, dataset_uri_list):\n",
    "                pair_list.append((query_uri, dataset_uri))\n",
    "            pbar.update(1)\n",
    "    # print('pair len:', len(pair_list))\n",
    "    with open(f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/pairs/{test_collection}_pairs.tsv', 'w+') as f:\n",
    "        for pair in pair_list:\n",
    "            f.write(f'{pair[0]}\\t{pair[1]}\\n')\n",
    "    print(f'finish {test_collection} pairs')\n",
    "pair_list = list(set(pair_list))\n",
    "with open(f'/home/xxx/code/keds/kgtk_similarity/pairs/all_pairs.tsv', 'w+') as f:\n",
    "    for pair in pair_list:\n",
    "        f.write(f'{pair[0]}\\t{pair[1]}\\n')\n",
    "print(f'finish {test_collection} pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/xxx/code/reproduce_keds/data/kgtk_similarity/pairs'\n",
    "with open(f'{data_path}/all_pairs.tsv', 'w+') as fp:\n",
    "    pairs = set()\n",
    "    for test_collection in ['acordar1', 'ntcir15', 'ntcir16']:\n",
    "        with open(f'{data_path}/{test_collection}_pairs.tsv', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line in pairs:\n",
    "                    continue\n",
    "                pairs.add(line)\n",
    "                fp.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "error_list = []\n",
    "def single_run(unhandled_list=None):\n",
    "    global target_list\n",
    "    pair_list = []\n",
    "    similarity_path = f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/similarity/slices'\n",
    "    pair_path = f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/pairs/all_pairs.tsv'\n",
    "    for row in csv.reader(open(pair_path, 'r'), delimiter='\\t'):\n",
    "        pair_list.append((row[0], row[1]))\n",
    "    \n",
    "    print('pair len:', len(pair_list))\n",
    "    l = 25\n",
    "\n",
    "    if unhandled_list:\n",
    "        for index in unhandled_list:\n",
    "            index = int(index)\n",
    "            with open('tmp.tsv', 'w+') as fp:\n",
    "                fp.write('q1\\tq2\\n')\n",
    "                for pair in pair_list[index:index+l]:\n",
    "                    fp.write(f'{pair[0]}\\t{pair[1]}\\n')\n",
    "            try:\n",
    "                s = call_semantic_similarity('tmp.tsv')\n",
    "                s = [dict(t) for t in set([tuple(d.items()) for d in s])]\n",
    "                assert len(s) == l\n",
    "            except Exception as e:\n",
    "                logging.error(f'Error in calling {url} with [{index}:{index+l}]: {e} len s: {len(s)}')\n",
    "                error_list.append(index)\n",
    "                s = []\n",
    "            target_list.extend(s)\n",
    "            logging.info(f'finish {index + l}')\n",
    "        json_dump(target_list, f'/home/xxx/code/keds/kgtk_similarity/similarity/all_unhandled_similarity.json')\n",
    "        return\n",
    "    \n",
    "    # pair_list = pair_list[110000:]\n",
    "    with tqdm(total=len(pair_list[110000:]), ncols=100, leave=True) as pbar:\n",
    "        for i in range(110000, len(pair_list), l):\n",
    "            with open('tmp.tsv', 'w+') as fp:\n",
    "                fp.write('q1\\tq2\\n')\n",
    "                for pair in pair_list[i:i+l]:\n",
    "                    fp.write(f'{pair[0]}\\t{pair[1]}\\n')\n",
    "            try:\n",
    "                s = call_semantic_similarity('tmp.tsv')\n",
    "                s = [dict(t) for t in set([tuple(d.items()) for d in s])]\n",
    "                assert len(s) == len(pair_list[i:i+l])\n",
    "            except Exception as e:\n",
    "                logging.error(f'Error in calling {url} with [{i}:{i+l}]: {e} len s: {len(s)}')\n",
    "                error_list.append(i)\n",
    "                s = []\n",
    "            target_list.extend(s)\n",
    "            logging.info(f'finish {i + l}')\n",
    "            if int(i + l) % 10000 == 0:\n",
    "                target_list = [dict(t) for t in set([tuple(d.items()) for d in target_list])]\n",
    "                json_dump(target_list[i + l - 10000: i + l], f'{similarity_path}/{i + l}.json')\n",
    "            pbar.update(l)\n",
    "    target_list = [dict(t) for t in set([tuple(d.items()) for d in target_list])]\n",
    "    json_dump(target_list, f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/similarity/all_similarity.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair len: 149099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290161acd696442e890adcdf7b61b7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                     | 0/39099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sim = []\n",
    "similarity_path = f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/similarity/slices'\n",
    "for file in os.listdir(similarity_path):\n",
    "    all_sim.extend(json_load(f'{similarity_path}/{file}'))\n",
    "\n",
    "json_dump(all_sim, f'/home/xxx/code/reproduce_keds/data/kgtk_similarity/similarity/all_similarity.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "unhandled_list = []\n",
    "with open('/home/xxx/code/reproduce_keds/kgtk_similarity.log', 'r+') as fp:\n",
    "    for line in fp.readlines():\n",
    "        if 'len s:' in line:\n",
    "            error = re.search(r'\\[\\d+:\\d+\\]', line).group(0)\n",
    "            unhandled_list.append(error[1:-1].split(':')[0])\n",
    "print(unhandled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_run(unhandled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_path = f'/home/xxx/code/keds/kgtk_similarity/similarity/{test_collection}_rank{rank}_similarity.json'\n",
    "src_path = '/home/xxx/code/keds/kgtk_similarity/similarity/all_similarity.json'\n",
    "\n",
    "data = json_load(src_path)\n",
    "row_list = []\n",
    "for item in data:\n",
    "    query_entity_uri = 'http://www.wikidata.org/entity/' + item['q1']\n",
    "    dataset_entity_uri = 'http://www.wikidata.org/entity/' + item['q2']\n",
    "    query_entity_label = item['q1_label'] if item['q1_label'] else ''\n",
    "    dataset_entity_label = item['q2_label'] if item['q2_label'] else ''\n",
    "    text_similarity = item['text'] if item['text'] else -1\n",
    "    row_list.append((query_entity_uri, dataset_entity_uri, query_entity_label, dataset_entity_label, float(text_similarity)))\n",
    "\n",
    "# ntcir_cursor_org.executemany(f'INSERT  INTO {test_collection}_query_dataset_entity_similartity_kgtk \\\n",
    "                        #  (query_entity_uri, dataset_entity_uri, query_entity_label, dataset_entity_label, text_similarity ) \\\n",
    "                        #  VALUES (%s,%s,%s,%s,%s);', row_list)\n",
    "cursor.executemany(f'INSERT  INTO query_dataset_entity_similartity_kgtk \\\n",
    "                         (query_entity_uri, dataset_entity_uri, query_entity_label, dataset_entity_label, text_similarity ) \\\n",
    "                         VALUES (%s,%s,%s,%s,%s);', row_list)\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}